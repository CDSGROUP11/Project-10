{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOoYdkRgK/U0jKaA5Q8c+0U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CDSGROUP11/Project-10/blob/UI_Feature/MainUI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XAOlAmyZD_xz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49733f1c-77c4-4675-985a-b87b1843367a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.4/74.4 kB\u001b[0m \u001b[31m549.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.0/48.0 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.9/417.9 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m875.6/875.6 kB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -qq install langchain_openai\n",
        "!pip -qq install PyPDF2\n",
        "!pip -qq install langchain_community\n",
        "!pip -qq install faiss-gpu-cu11\n",
        "!pip -qq install streamlit pyngrok\n",
        "!pip -qq install streamlit_modal"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from PyPDF2 import PdfReader\n",
        "import os"
      ],
      "metadata": {
        "id": "R4QcV0dPHSNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.docstore.document import Document\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "import streamlit as st"
      ],
      "metadata": {
        "id": "HPVBiHjZId8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "!ngrok config add-authtoken <your token>"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSFvzyeSU4qB",
        "outputId": "1e528899-996c-4c8f-e4db-eb136130c015"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile main.py\n",
        "import streamlit as st\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from google.colab import userdata\n",
        "from datetime import datetime\n",
        "import os\n",
        "from streamlit_modal import Modal\n",
        "\n",
        "# Ensure GITHUB_TOKEN is set as an environment variable for use in ChatOpenAI\n",
        "github_token = <your token>\n",
        "os.environ['GITHUB_TOKEN'] = github_token\n",
        "endpoint = \"https://models.github.ai/inference\"\n",
        "model_name = \"openai/gpt-4.1-nano\"\n",
        "\n",
        "def get_pdf_text(pdf):\n",
        "    text = \"\"\n",
        "    pdf_reader = PdfReader(pdf)\n",
        "    for page in pdf_reader.pages:\n",
        "        text+= page.extract_text()\n",
        "    return text\n",
        "\n",
        "def get_text_chunks(text):\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 10000,\n",
        "    chunk_overlap = 100\n",
        "    )\n",
        "    chunks = text_splitter.split_text(text)\n",
        "    return chunks\n",
        "\n",
        "def get_vector_store(text_chunks):\n",
        "    embeddings = OpenAIEmbeddings(\n",
        "        model='text-embedding-3-small',\n",
        "        dimensions=1536,\n",
        "        base_url = endpoint,\n",
        "        api_key= github_token)\n",
        "    vector_store = FAISS.from_texts(text_chunks, embedding=embeddings)\n",
        "    vector_store.save_local(\"faiss_index\")\n",
        "\n",
        "\n",
        "def get_conversation_chain():\n",
        "    model = ChatOpenAI(\n",
        "    base_url = endpoint,\n",
        "    api_key= github_token,\n",
        "    model=model_name,\n",
        "    temperature=0.1\n",
        "    )\n",
        "    chat_prompt_template = ChatPromptTemplate([\n",
        "         ('system', 'You are good resume parser expert and helps in answering questions. Answer the question from the provided resume. Make sure to provide all the details, if the answer is not in the provided resume just reply back, \"Answer is not available in provided resume\", dont provide the wrong answers or dont hallucinate the answers.\\n Resume: \\n {resume}?'),\n",
        "         ('human', 'Question: \\n {question}\\nAnswer:')\n",
        "        ])\n",
        "    chain = chat_prompt_template|model|StrOutputParser()\n",
        "    return chain\n",
        "\n",
        "def notice_period_prompt():\n",
        "    model = ChatOpenAI(\n",
        "    base_url = endpoint,\n",
        "    api_key= github_token,\n",
        "    model=model_name,\n",
        "    temperature=0.1\n",
        "    )\n",
        "    chat_prompt_template = ChatPromptTemplate([\n",
        "         ('system', 'You are good and helpful assistant and helps in answering questions. Find out the notice period from public available sources for most recent organization provided in resume. Make sure to provide to provide accurate details and provide the source link at the end of answer, dont provide the wrong answers or dont hallucinate the answers. \\n Resume: \\n {resume}?'),\n",
        "         ('human', 'Question: \\n Notice Period of the most recent job organization the candidate working for. \\nAnswer:')\n",
        "        ])\n",
        "    chain = chat_prompt_template|model|StrOutputParser()\n",
        "    return chain\n",
        "\n",
        "def handle_notice_period():\n",
        "    user_question = \"Notice Period of the most recent job organization the candidate working for\"\n",
        "    embeddings = OpenAIEmbeddings(\n",
        "        model='text-embedding-3-small',\n",
        "        dimensions=1536,\n",
        "        base_url = endpoint,\n",
        "        api_key= github_token,)\n",
        "    new_db = FAISS.load_local(\"faiss_index\", embeddings=embeddings, allow_dangerous_deserialization=True)\n",
        "    docs = new_db.similarity_search(user_question)\n",
        "    chain = notice_period_prompt()\n",
        "    response = chain.invoke({\n",
        "        \"resume\": docs\n",
        "    })\n",
        "    return response\n",
        "\n",
        "def handle_user_input(user_question):\n",
        "    embeddings = OpenAIEmbeddings(\n",
        "        model='text-embedding-3-small',\n",
        "        dimensions=1536,\n",
        "        base_url = endpoint,\n",
        "        api_key= github_token,)\n",
        "    new_db = FAISS.load_local(\"faiss_index\", embeddings=embeddings, allow_dangerous_deserialization=True)\n",
        "    docs = new_db.similarity_search(user_question)\n",
        "    chain = get_conversation_chain()\n",
        "    response = chain.invoke({\n",
        "        \"resume\": docs,\n",
        "        \"question\": user_question\n",
        "    })\n",
        "    return response\n",
        "\n",
        "st.set_page_config(\"Pre-Screening Intelligence: TalAI\", layout=\"wide\")\n",
        "\n",
        "left_col, middle_col, right_col = st.columns([1, 1, 1])\n",
        "\n",
        "# Initialize modal\n",
        "resume_modal = Modal(\"📂 Resume Tools\", key=\"resume-tools-modal\", max_width=\"700px\")\n",
        "\n",
        "# with left_col:\n",
        "#     st.header(\"📂 Resume Tools\")\n",
        "#     pdf_doc = st.file_uploader(\"Upload PDF Resume\")\n",
        "#     if st.button(\"Submit and Process\") and pdf_doc:\n",
        "#         with st.spinner(\"Processing...\"):\n",
        "#             raw_text = get_pdf_text(pdf_doc)\n",
        "#             text_chunks = get_text_chunks(raw_text)\n",
        "#             get_vector_store(text_chunks)\n",
        "#             st.success(\"Resume processed and indexed!\")\n",
        "#             st.session_state.chat_history = []\n",
        "\n",
        "section_list = [\n",
        "    'Work Experience',\n",
        "    'Education',\n",
        "    'Skills',\n",
        "    'Academic Projects',\n",
        "    'Certification, Award & Recognition',\n",
        "    'Career Objectives or Summary',\n",
        "    'Personal Details & Hobbies'\n",
        "]\n",
        "\n",
        "with left_col:\n",
        "    # Upload button triggers modal\n",
        "    if st.button(\"📤 Upload Resumes\"):\n",
        "        resume_modal.open()\n",
        "\n",
        "    # Modal content\n",
        "    if resume_modal.is_open():\n",
        "        with resume_modal.container():\n",
        "            st.markdown(\"\"\"\n",
        "            <div style=\"display: flex; flex-direction: column; align-items: center; text-align: center;\">\n",
        "                <img src=\"https://raw.githubusercontent.com/surabhi13gupta/LangChains/main/TalAI.png\" width=\"60\" style=\"border-radius: 50%; margin-bottom: 1rem;\">\n",
        "                <h4 style=\"margin-bottom: 0;\">TalAI ResumeTools</h4>\n",
        "                <p style=\"margin-top: 0;\">Upload your resume to extract insights, skills, and suggestions.</p>\n",
        "                </div>\"\"\", unsafe_allow_html=True)\n",
        "\n",
        "            # Resume upload\n",
        "            uploaded_file = st.file_uploader(\"Upload Resume (PDF/DOCX are allowed)\", type=[\"pdf\", \"docx\"])\n",
        "\n",
        "            if st.button(\"Submit and Process\") and uploaded_file:\n",
        "                with st.spinner(\"Processing...\"):\n",
        "                    raw_text = get_pdf_text(uploaded_file)\n",
        "                    text_chunks = get_text_chunks(raw_text)\n",
        "                    get_vector_store(text_chunks)\n",
        "                    st.success(\"Resume uploaded, processed and indexed!\")\n",
        "                    st.session_state.chat_history = []\n",
        "                    st.button(\"✅ Close\", on_click=lambda: resume_modal.close())\n",
        "            else:\n",
        "                st.info(\"Awaiting file upload...\")\n",
        "\n",
        "    st.header(\"📊 Top 5 Resumes\")\n",
        "\n",
        "    # Job description input\n",
        "    job_description = st.text_area(\n",
        "        \"📝 Job Description\",\n",
        "        height=200,\n",
        "        placeholder=\"Paste the job role or requirements here...\"\n",
        "    )\n",
        "\n",
        "    for section in section_list:\n",
        "        with st.expander(f\"📂 {section}\"):\n",
        "            st.text_area(f\"✏️ Edit or review: {section}\", height=150, placeholder=f\"Enter details for {section}...\")\n",
        "\n",
        "    # Submit button\n",
        "    if st.button(\"🚀 Match Resumes\") and job_description:\n",
        "        with st.spinner(\"Matching resumes...\"):\n",
        "            # Placeholder for vector search logic\n",
        "            top_matches = [\n",
        "                \"**1. Jane Doe** — ML Engineer, 5 yrs exp, NLP-heavy projects\",\n",
        "                \"**2. Ravi Kumar** — Time Series Specialist, fintech background\",\n",
        "                \"**3. Aisha Rahman** — GenAI pipeline builder, LangChain expert\",\n",
        "                \"**4. Leo Zhang** — Resume parsing wizard, UX-focused\",\n",
        "                \"**5. Sara Ali** — Dashboard designer, Streamlit + LLM integration\"\n",
        "            ]\n",
        "\n",
        "            # Display results\n",
        "            st.markdown(\"### 🏆 Top Resume Matches\")\n",
        "            for match in top_matches:\n",
        "                st.markdown(f\"- {match}\")\n",
        "\n",
        "with middle_col:\n",
        "    st.header(\"🎥 Video Summarization\")\n",
        "    st.button(\"Summarize Video\")\n",
        "\n",
        "with right_col:\n",
        "    st.header(\"💬 Chat with TalAI\")\n",
        "    if \"chat_history\" not in st.session_state:\n",
        "        st.session_state.chat_history = []\n",
        "\n",
        "    for msg in st.session_state.chat_history:\n",
        "        role = \"user\" if msg[\"role\"] == \"user\" else \"assistant\"\n",
        "        st.chat_message(role).write(msg[\"content\"])\n",
        "\n",
        "    chat_container = st.container()\n",
        "\n",
        "    # Handle input first\n",
        "    first_question = \"Candidate Name, total years of experience and recent job organisation\"\n",
        "    st.session_state.chat_history.append({\n",
        "            \"role\": \"user\",\n",
        "            \"content\": first_question\n",
        "        })\n",
        "    recent_job = handle_user_input(first_question)\n",
        "    st.session_state.chat_history.append({\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": recent_job\n",
        "    })\n",
        "\n",
        "    st.session_state.chat_history.append({\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Notice Period information for recent company\"\n",
        "        })\n",
        "    notice_response = handle_notice_period()\n",
        "    st.session_state.chat_history.append({\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": notice_response\n",
        "    })\n",
        "\n",
        "    user_question = st.chat_input(\"Ask TalAI about the selected resume...\")\n",
        "    if user_question:\n",
        "        st.session_state.chat_history.append({\n",
        "            \"role\": \"user\",\n",
        "            \"content\": user_question\n",
        "        })\n",
        "        response = handle_user_input(user_question)\n",
        "        st.session_state.chat_history.append({\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": response\n",
        "        })\n",
        "\n",
        "    # Render all messages inside chat_container\n",
        "    with chat_container:\n",
        "        for msg in (st.session_state.chat_history):  # Latest at top\n",
        "            role = \"user\" if msg[\"role\"] == \"user\" else \"assistant\"\n",
        "            prefix = \"You: \" if role == \"user\" else \"TalAI: \"\n",
        "            st.chat_message(role).write(prefix + msg[\"content\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7H8Slv3u9ku_",
        "outputId": "67888819-00a4-419a-e2e9-50845a393604"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing main.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "public_url = ngrok.connect(addr=8501)\n",
        "\n",
        "print(f\"Streamlit URL: {public_url}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "7rPY3Nlx9q4W",
        "outputId": "f968cee1-82d7-4b61-cae6-0de034297083"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:pyngrok.process.ngrok:t=2025-08-21T17:00:48+0000 lvl=eror msg=\"failed to reconnect session\" obj=tunnels.session err=\"authentication failed: Your account is limited to 1 simultaneous ngrok agent sessions.\\nYou can run multiple simultaneous tunnels from a single agent session by defining the tunnels in your agent configuration file and starting them with the command `ngrok start --all`.\\nRead more about the agent configuration file: https://ngrok.com/docs/secure-tunnels/ngrok-agent/reference/config\\nYou can view your current agent sessions in the dashboard:\\nhttps://dashboard.ngrok.com/agents\\r\\n\\r\\nERR_NGROK_108\\r\\n\"\n",
            "ERROR:pyngrok.process.ngrok:t=2025-08-21T17:00:48+0000 lvl=eror msg=\"session closing\" obj=tunnels.session err=\"authentication failed: Your account is limited to 1 simultaneous ngrok agent sessions.\\nYou can run multiple simultaneous tunnels from a single agent session by defining the tunnels in your agent configuration file and starting them with the command `ngrok start --all`.\\nRead more about the agent configuration file: https://ngrok.com/docs/secure-tunnels/ngrok-agent/reference/config\\nYou can view your current agent sessions in the dashboard:\\nhttps://dashboard.ngrok.com/agents\\r\\n\\r\\nERR_NGROK_108\\r\\n\"\n",
            "ERROR:pyngrok.process.ngrok:t=2025-08-21T17:00:48+0000 lvl=eror msg=\"terminating with error\" obj=app err=\"authentication failed: Your account is limited to 1 simultaneous ngrok agent sessions.\\nYou can run multiple simultaneous tunnels from a single agent session by defining the tunnels in your agent configuration file and starting them with the command `ngrok start --all`.\\nRead more about the agent configuration file: https://ngrok.com/docs/secure-tunnels/ngrok-agent/reference/config\\nYou can view your current agent sessions in the dashboard:\\nhttps://dashboard.ngrok.com/agents\\r\\n\\r\\nERR_NGROK_108\\r\\n\"\n",
            "CRITICAL:pyngrok.process.ngrok:t=2025-08-21T17:00:48+0000 lvl=crit msg=\"command failed\" err=\"authentication failed: Your account is limited to 1 simultaneous ngrok agent sessions.\\nYou can run multiple simultaneous tunnels from a single agent session by defining the tunnels in your agent configuration file and starting them with the command `ngrok start --all`.\\nRead more about the agent configuration file: https://ngrok.com/docs/secure-tunnels/ngrok-agent/reference/config\\nYou can view your current agent sessions in the dashboard:\\nhttps://dashboard.ngrok.com/agents\\r\\n\\r\\nERR_NGROK_108\\r\\n\"\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "PyngrokNgrokError",
          "evalue": "The ngrok process errored on start: authentication failed: Your account is limited to 1 simultaneous ngrok agent sessions.\\nYou can run multiple simultaneous tunnels from a single agent session by defining the tunnels in your agent configuration file and starting them with the command `ngrok start --all`.\\nRead more about the agent configuration file: https://ngrok.com/docs/secure-tunnels/ngrok-agent/reference/config\\nYou can view your current agent sessions in the dashboard:\\nhttps://dashboard.ngrok.com/agents\\r\\n\\r\\nERR_NGROK_108\\r\\n.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPyngrokNgrokError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3972987130.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpublic_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mngrok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8501\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Streamlit URL: {public_url}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyngrok/ngrok.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(addr, proto, name, pyngrok_config, **options)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Opening tunnel named: {name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m     \u001b[0mapi_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ngrok_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Creating tunnel with options: {options}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyngrok/ngrok.py\u001b[0m in \u001b[0;36mget_ngrok_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0minstall_ngrok\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyngrok/process.py\u001b[0m in \u001b[0;36mget_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_current_processes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mngrok_path\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_start_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyngrok/process.py\u001b[0m in \u001b[0;36m_start_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mngrok_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartup_error\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m             raise PyngrokNgrokError(f\"The ngrok process errored on start: {ngrok_process.startup_error}.\",\n\u001b[0m\u001b[1;32m    448\u001b[0m                                     \u001b[0mngrok_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m                                     ngrok_process.startup_error)\n",
            "\u001b[0;31mPyngrokNgrokError\u001b[0m: The ngrok process errored on start: authentication failed: Your account is limited to 1 simultaneous ngrok agent sessions.\\nYou can run multiple simultaneous tunnels from a single agent session by defining the tunnels in your agent configuration file and starting them with the command `ngrok start --all`.\\nRead more about the agent configuration file: https://ngrok.com/docs/secure-tunnels/ngrok-agent/reference/config\\nYou can view your current agent sessions in the dashboard:\\nhttps://dashboard.ngrok.com/agents\\r\\n\\r\\nERR_NGROK_108\\r\\n."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run main.py&"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olrmCFD89wyA",
        "outputId": "8e74e7b6-ad6d-41f1-f6cc-62cf8d2ceaf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.147.78.251:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Stopping...\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "load_qa_chain is a LangChain helper that loads a Chain designed for answering questions based on provided documents. It wraps a language model (LLM) with a prompt template and logic for combining document context and user queries"
      ],
      "metadata": {
        "id": "QJmRpLhGK0j5"
      }
    }
  ]
}